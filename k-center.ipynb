{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da838d5d-b385-4a87-a2b2-68a8936cac42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pulp in /Users/yueyang/anaconda3/lib/python3.11/site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1eee129-6e69-4beb-8630-c05f9d728cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import pulp\n",
    "from sklearn.datasets import make_blobs\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21117251-89ca-4ac4-9817-20b56078fa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################# set up parameters ###################################################\n",
    "beta = 1\n",
    "epsilon = 0.25\n",
    "\n",
    "k = 5\n",
    "alpha = 2 * np.sqrt(2)\n",
    "delta = np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ee672d2-8472-4534-93ba-20a4f44b9d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################ useful helpers ###################################################\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def calculate_diameter(points):\n",
    "    diameter = 0.0\n",
    "    num_points = len(points)\n",
    "    for i in range(num_points):\n",
    "        for j in range(i+1, num_points):\n",
    "            distance = euclidean_distance(points[i], points[j])\n",
    "            if distance > diameter:\n",
    "                diameter = distance\n",
    "    return diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d7794ce-eb77-4995-b4bb-3767c2f78240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################### offline algorithm to produce offline OPT distance #################################\n",
    "# greedy approximation to compute OPT_distance\n",
    "def offline_k_center(points, k):\n",
    "    # Initialize the first center randomly\n",
    "    centers = [points[np.random.randint(len(points))]]\n",
    "    \n",
    "    while len(centers) < k:\n",
    "        # Find the point that is the farthest from any center\n",
    "        next_center = max(points, key=lambda point: min(euclidean_distance(point, center) for center in centers))\n",
    "        centers.append(next_center)\n",
    "    \n",
    "    return centers\n",
    "\n",
    "# LP relaxation for offline k-center's OPT_distance \n",
    "def lp_relaxation_k_center(points, k):\n",
    "    num_points = len(points)\n",
    "    prob = pulp.LpProblem(\"k_Center\", pulp.LpMinimize)\n",
    "\n",
    "    # populate distance matrix\n",
    "    dist_mat = np.zeros((num_points, num_points))\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            dist_mat[i, j] = euclidean_distance(points[i], points[j])\n",
    "    #print(\"distance matrix:\", dist_mat)\n",
    "\n",
    "    # Variables\n",
    "    x = pulp.LpVariable.dicts(\"x\", (range(num_points), range(num_points)), lowBound=0, upBound=1, cat=pulp.LpContinuous)\n",
    "    y = pulp.LpVariable.dicts(\"y\", range(num_points), lowBound=0, upBound=1, cat=pulp.LpContinuous)\n",
    "    z = pulp.LpVariable(\"z\", lowBound=0, cat=pulp.LpContinuous)\n",
    "\n",
    "    # Objective\n",
    "    prob += z, \"Maximum distance to nearest center\"\n",
    "\n",
    "    # Constraints\n",
    "    for i in range(num_points):\n",
    "        prob += pulp.lpSum(x[i][j] for j in range(num_points)) == 1, f\"Assign_{i}\"\n",
    "        for j in range(num_points):\n",
    "            prob += x[i][j] <= y[j], f\"Link_{i}_{j}\"\n",
    "            prob += dist_mat[i][j] * x[i][j] <= z, f\"Distance_{i}_{j}\"\n",
    "\n",
    "    prob += pulp.lpSum(y[j] for j in range(num_points)) == k, \"Number_of_centers\"\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "    \n",
    "    # print out the x matrix for debugging\n",
    "    x_mat = np.zeros((num_points, num_points))\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            x_mat [i, j] = x[i][j].varValue\n",
    "\n",
    "    z = np.zeros(num_points)\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            z[i] += x_mat[i][j] * dist_mat[i][j]\n",
    "    \n",
    "    result = np.max(z)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# used in conjunction with greedy approximation to calculate the max distance \n",
    "# from a point to its nearest center\n",
    "def max_distance_to_centers(points, centers):\n",
    "    max_dist = 0\n",
    "    for point in points:\n",
    "        min_dist_to_center = min(euclidean_distance(point, center) for center in centers)\n",
    "        max_dist = max(max_dist, min_dist_to_center)\n",
    "    return max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31155a55-740c-4776-ad1d-600eb4d77941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_points_and_centers(points, centers):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    x, y = zip(*points)\n",
    "    cx, cy = zip(*centers)\n",
    "    \n",
    "    plt.scatter(x, y, color='blue', label='Points')\n",
    "    plt.scatter(cx, cy, color='red', s=100, label='Centers', edgecolors='black')\n",
    "    plt.title(\"Offline k-Center Clustering\")\n",
    "    plt.xlabel(\"X coordinate\")\n",
    "    plt.ylabel(\"Y coordinate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "261691c6-421e-4bd9-9952-3bb30b2ab824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################### online update algorithm for k-center ##################################################\n",
    "\n",
    "####################################### functions needed for main method ############################################################\n",
    "\n",
    "# update the covering constraints to accommodate insertion and deletion of clients\n",
    "# (potentially needed)\n",
    "def update_constraints(C,s, n):\n",
    "    # update the constraint matrix that for each client, Cx >= 1\n",
    "    # where row t in C for client t has 0s at entries in s\n",
    "    new_row = np.zeros(n)\n",
    "    new_row[s] = 1\n",
    "    C.append(new_row)\n",
    "    \n",
    "    return C\n",
    "\n",
    "# find all points within the radius of the current client\n",
    "# this step can be thought of as updating the covering constraint\n",
    "def find_candidates(client_indices, points, client, radius):\n",
    "    s = []\n",
    "    for index in client_indices:\n",
    "        if euclidean_distance(points[int(index)], client) <= radius:\n",
    "            s.append(int(index))\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "# check if new client's constraint is satisfied\n",
    "def check_covering_feasibility(s, x):\n",
    "    # check if the covering constraint is satisfied\n",
    "    covering_sum = np.sum(x[s])\n",
    "    if covering_sum >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# alternative method to update x's when a covering violation occurs\n",
    "# we use the closed form with the lagrange multiplier here\n",
    "def update_covering(x, s, epsilon):\n",
    "    d = len(s)\n",
    "    log_term = (epsilon/4 + 1) / (np.sum(x[s]) + epsilon / 4)\n",
    "\n",
    "    y = np.log(log_term)\n",
    "    x[s] = (x[s] + epsilon / (4 * d)) * np.exp(y) - (epsilon / (4 * d))\n",
    "    for i in s:\n",
    "        x[i] = max(0, x[i])\n",
    "\n",
    "    return x\n",
    "\n",
    "# alternative method to update x's when a packing violation occurs\n",
    "# similarly, we also use the closed form here\n",
    "def update_packing(x, epsilon, k):\n",
    "    log_term_bottom = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] > 0:\n",
    "            log_term_bottom += x[i]\n",
    "    \n",
    "    if log_term_bottom > 0:\n",
    "        neg_z = np.log(((1 + epsilon) * k) / log_term_bottom)\n",
    "        x = x * np.exp(neg_z)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "efec41cf-52e0-4947-ad93-90c0fa3145d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################ method used to compute OPT_rec at time t #################################\n",
    "\n",
    "# arguments: \n",
    "#   C: a list of length T that consists of covering constraints at time t; \n",
    "#      C(t) is the indices of non-zero c_i's that appear in a covering constraint at time t: \n",
    "#   P: a list of packing contraints at each t defined similarly to C\n",
    "#   t: index of current iteration (starts from 0)\n",
    "def compute_OPT_rec(C_list, P_list, t, k, epsilon, client_indices):\n",
    "\n",
    "    # Problem data and parameters\n",
    "    T = t + 1  # Number of time periods\n",
    "    n = t + 1  # Number of total variables (including removed clients)\n",
    "    # Create the LP problem object\n",
    "    lp_prob = pulp.LpProblem(\"OPT_recourse\", pulp.LpMinimize)\n",
    "\n",
    "    # Decision variables x_i^t and l_i^t\n",
    "    x = pulp.LpVariable.dicts(\"x\", (range(T), range(n)), lowBound=0, upBound=1, cat=pulp.LpContinuous)\n",
    "    l = pulp.LpVariable.dicts(\"l\", (range(T), range(n)), lowBound=0, cat=pulp.LpContinuous)\n",
    "    #z = pulp.LpVariable(\"z\", lowBound=0)\n",
    "\n",
    "    # Objective function\n",
    "    lp_prob += pulp.lpSum(l[t][i] for i in range(n) for t in range(T))\n",
    "\n",
    "    # Constraints\n",
    "    for t in range(len(C_list)):\n",
    "        C_t = C_list[t]\n",
    "        for c in range(len(C_t)):\n",
    "            subset_indices = C_t[c]\n",
    "            lp_prob += pulp.lpSum(x[t][i] for i in subset_indices) >= 1  \n",
    "    for t in range(len(P_list)):\n",
    "        P = P_list[t]\n",
    "        \n",
    "        #lp_prob += z <= (1 + epsilon) * k\n",
    "        #lp_prob += z <= len(P)\n",
    "        lp_prob += pulp.lpSum(x[t][i] for i in P) <= (1 + epsilon) * k  \n",
    "        # make sure that removed points are set to 0\n",
    "        for i in range(len(client_indices)):\n",
    "            if i not in P:\n",
    "                lp_prob += x[t][i] == 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # include the recourse at t = 0\n",
    "        lp_prob += x[0][i] <= l[0][i]\n",
    "    for t in range(1, T):\n",
    "        for i in range(n):\n",
    "            lp_prob += (x[t][i] - x[t-1][i]) <= l[t][i] #, f\"ChangeConstraint_Pos_{t}_{i}\"\n",
    "            lp_prob += (x[t-1][i] - x[t][i]) <= l[t][i] #, f\"ChangeConstraint_Neg_{t}_{i}\"\n",
    "\n",
    "    # Solve the problem\n",
    "    lp_prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "    # Output results\n",
    "    # printing x and l matrices for debugging\n",
    "    x_mat = np.zeros((T, n))\n",
    "    l_mat = np.zeros((T, n))\n",
    "\n",
    "    for i in range(T):\n",
    "        for j in range(n):\n",
    "            x_mat [i, j] = x[i][j].varValue\n",
    "            l_mat[i, j] = l[i][j].varValue\n",
    "    \n",
    "    # return the last row of the x matrix as the fractional solution for rounding\n",
    "    return pulp.value(lp_prob.objective), x_mat[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0317e922-5380-4bef-9185-278184a408c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################################### helper for rounding ###########################################\n",
    "\n",
    "# subroutine to find the balls B_i and B_hait_i given the identifying index of a center\n",
    "# this subroutine is called whenever the set S(t) is updated\n",
    "def find_balls(data_points, client_indices, center_index, radius, radius_hat):\n",
    "\n",
    "    B_i = []\n",
    "    B_i_hat =[]\n",
    "\n",
    "    for index in client_indices:\n",
    "        if euclidean_distance(data_points[index], data_points[center_index]) <= radius:\n",
    "            B_i.append(index)\n",
    "            \n",
    "        if euclidean_distance(data_points[index], data_points[center_index]) <= radius_hat:\n",
    "            B_i_hat.append(index)\n",
    "    \n",
    "    return B_i, B_i_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ecdce8d7-d281-41a3-9eb8-f69f192b4469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################### main method for online k-center #######################################\n",
    "def online_k_center(requests, points, k):\n",
    "\n",
    "    total_recourse = 0\n",
    "    total_integer_recourse = 0\n",
    "\n",
    "    # initialize the vector x with all 0s of dimension len(points)\n",
    "    x = np.zeros(len(points))\n",
    "\n",
    "    # store each client as a tuple (index in points, coordinates)\n",
    "    clients = []\n",
    "    # the coordinates of active clients\n",
    "    client_points = []\n",
    "    client_indices = []\n",
    "\n",
    "    set_of_centers = []\n",
    "    radius_of_centers = np.zeros(len(x))\n",
    "\n",
    "    # initialize variables needed for computing OPT_rec:\n",
    "    # a t-by-n list of lists C, whose number of rows grows as t increases,\n",
    "    # each row stores the covering constraints of time t\n",
    "    # A matrix P for packing constraints is defined similarly\n",
    "    C_list = []\n",
    "    P_list = []\n",
    "\n",
    "    t = 0  # for indexing the data points     \n",
    "    for r in range(len(requests)):\n",
    "\n",
    "        x_old = np.copy(x)\n",
    "\n",
    "        # if requests[r] == 1, then the current request is a removal\n",
    "        # remove the randomly sampled client from the set of active clients\n",
    "        if requests[r] == -1:\n",
    "            # random sample an active client to remove\n",
    "            if len(client_indices) and len(client_indices) > len(set_of_centers) > 0:\n",
    "                client_to_remove = random.randint(0, t)\n",
    "                while client_to_remove in set_of_centers and client_to_remove not in client_indices:\n",
    "                    client_to_remove = random.randint(0, t)\n",
    "                \n",
    "                x[client_to_remove] = 0\n",
    "                if client_to_remove in client_indices:\n",
    "                    client_indices.remove(client_to_remove)\n",
    "                \n",
    "                    item_to_remove = points[client_to_remove]\n",
    "                    for i in range(len(client_points)):\n",
    "                        if np.array_equal(client_points[i], item_to_remove):\n",
    "                            client_points.pop(i)\n",
    "                            break\n",
    "\n",
    "                    for client in clients:\n",
    "                        if client[0] == client_to_remove:\n",
    "                            clients.remove(client)\n",
    "            continue\n",
    "\n",
    "        # otherwise, the request is an insertion, add the new client to the set of active clients\n",
    "        current_client = (t, points[t])\n",
    "        current_client_coordinates = points[t]\n",
    "        clients.append(current_client)\n",
    "        client_points.append(points[t])\n",
    "        client_indices.append(t)\n",
    "\n",
    "        # calculate OPT_dist at this round\n",
    "        diam = calculate_diameter(client_points)\n",
    "        centers_offline = offline_k_center(client_points, k)\n",
    "        approx_dist = max_distance_to_centers(client_points, centers_offline)\n",
    "        current_OPT_dist = lp_relaxation_k_center(client_points, k)\n",
    "        current_OPT_dist = approx_dist \n",
    "\n",
    "        # stores all covering constraints at t for computing OPT_rec\n",
    "        C_t =[]\n",
    "\n",
    "        # find all points within min(beta * current_OPT_dist, diam) of the new client\n",
    "        s = find_candidates(client_indices, points, current_client_coordinates, min(beta * current_OPT_dist, diam))\n",
    "    \n",
    "        # check if covering constraint is violated (for the new client)\n",
    "        # if so, update the values of x's according to B_j(t) of each active client\n",
    "        if check_covering_feasibility(s, x) == False:\n",
    "            # covering constraint not satisfied, need to update values of x's in s\n",
    "            # add the set s to C_list for computing OPT_rec\n",
    "            x_new = update_covering(x, s, epsilon)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # check if packing constraint is violated\n",
    "        # if so, update the values of x's\n",
    "        if (np.sum(x_new) > (1+epsilon) * k):\n",
    "            x_new = update_packing(x_new, epsilon, k)\n",
    "\n",
    "        # record the packing constraint for this iteration\n",
    "        # this is used to compute OPT_rec\n",
    "        active_set = []\n",
    "        active_set = copy.deepcopy(client_indices)\n",
    "        P_list.append(active_set)\n",
    "\n",
    "        # recheck again the covering constraint's for all points \n",
    "        # formulate a covering constraint matrix\n",
    "        A_t = np.zeros((len(points), len(points)))\n",
    "        for client_index in client_indices:\n",
    "            s_set = find_candidates(client_indices, points, points[client_index], min(beta * current_OPT_dist, diam))\n",
    "            C_t.append(s_set)\n",
    "            A_t[client_index][s_set] = 1\n",
    "\n",
    "        A_t_x = np.dot(A_t, x_new)\n",
    "        while np.all(A_t_x[client_indices] >= 1 - 0.001) == False:\n",
    "            for client_index in client_indices:\n",
    "                s_set = find_candidates(client_indices, points, points[client_index], min(beta * current_OPT_dist, diam))\n",
    "                x_new = update_covering(x_new, s_set, epsilon)\n",
    "            A_t_x_new = np.dot(A_t, x_new)\n",
    "            if (np.all(A_t_x_new - A_t_x == 0)):\n",
    "                # avoid infinite loop\n",
    "                break\n",
    "            A_t_x = copy.deepcopy(A_t_x_new)\n",
    "        \n",
    "        for client_index in client_indices:\n",
    "            s_set = find_candidates(client_indices, points, points[client_index], min(beta * current_OPT_dist, diam))\n",
    "            if check_covering_feasibility(s_set, x_new) == False:\n",
    "                update_covering(x_new, s_set, epsilon)\n",
    "        \n",
    "        # record all covering constraints for this round\n",
    "        C_list.append(C_t)\n",
    "    \n",
    "        OPT_recourse, x_OPT = compute_OPT_rec(C_list, P_list, t, k, epsilon, client_indices)\n",
    "\n",
    "        # update total recourse in l1-norm\n",
    "        total_recourse += np.sum(np.abs(x_new - x_old))\n",
    "        x = x_new\n",
    "        \n",
    "        #################################### rounding procedure begins from here ####################################\n",
    "        # integrate rounding at each round\n",
    "        # maintain a set S (initially empty) of open centers at each t\n",
    "        # r_i of each 'center' is stored at radius_of_centers[center]\n",
    "        \n",
    "        # identify the balls/set of points that are B_i and B_i_hat for each i in set_of_centers\n",
    "        # while building the balls B_i, drop any i from set_of_centers if it has mass less than 1-epsilon\n",
    "\n",
    "        S_prev = copy.deepcopy(set_of_centers)\n",
    "        \n",
    "        list_of_B_i_hat = []\n",
    "        S = copy.deepcopy(set_of_centers)\n",
    "        for center in S:\n",
    "\n",
    "            if center not in client_indices: # this means 'center' was a removed point at some t\n",
    "                set_of_centers.remove(center)\n",
    "                #total_integer_recourse += 1\n",
    "                continue\n",
    "            \n",
    "            B_i, B_i_hat = find_balls(data_points, client_indices, center, radius_of_centers[center], alpha * min(beta * current_OPT_dist, diam))\n",
    "            \n",
    "            # drop any B_i whose mass is too small\n",
    "            mass = 0\n",
    "            for index_of_point in B_i:\n",
    "                mass += x_OPT[index_of_point]  # currently using the solution returned by OPT_rec for rounding\n",
    "\n",
    "            if mass < 1 - epsilon:\n",
    "                set_of_centers.remove(center)\n",
    "                #total_integer_recourse += 1\n",
    "            else:\n",
    "                list_of_B_i_hat.append(B_i_hat)\n",
    "        \n",
    "        covered_points = set(item for sublist in list_of_B_i_hat for item in sublist)\n",
    "        while len(covered_points) < len(client_indices):\n",
    "        # find the clients that are not covered by the current set of centers\n",
    "            uncovered = set(client_indices) - covered_points\n",
    "            j = next(iter(uncovered))\n",
    "            set_of_centers.append(j)\n",
    "            set_of_centers = list(set(set_of_centers)) # get rid of (potential) duplicates in the list\n",
    "            #total_integer_recourse += 1\n",
    "            # record current_radius\n",
    "            current_r = min(beta * current_OPT_dist, diam)\n",
    "            radius_of_centers[j] = current_r\n",
    "\n",
    "            S = copy.deepcopy(set_of_centers)\n",
    "            for center_index in S:\n",
    "                ball_dist = radius_of_centers[j] + radius_of_centers[center_index] + delta * min(radius_of_centers[j], radius_of_centers[center_index])\n",
    "                if center_index != j and euclidean_distance(points[center_index], points[j]) <= ball_dist:\n",
    "                    set_of_centers.remove(center_index)\n",
    "                    #total_integer_recourse += 1\n",
    "                    \n",
    "            # update the the set of B_i_hats\n",
    "            list_of_B_i_hat = []\n",
    "            for center in set_of_centers:\n",
    "                B_i, B_i_hat = find_balls(points, client_indices, center, radius_of_centers[center], alpha * min(beta * current_OPT_dist, diam))\n",
    "                list_of_B_i_hat.append(B_i_hat)\n",
    "            covered_points = set(item for sublist in list_of_B_i_hat for item in sublist)\n",
    "        \n",
    "        # compute rounding recourse this round\n",
    "        diff = set(S_prev) ^ set(set_of_centers)\n",
    "        total_integer_recourse += len(diff)\n",
    "\n",
    "        t += 1\n",
    "        final_set = client_points\n",
    "\n",
    "    return x, total_recourse, set_of_centers, OPT_recourse, total_integer_recourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df9359fb-b44f-4148-b1d3-25bf0e977075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name     role         type demographic       description  \\\n",
      "0       Id_number       ID      Integer        None              None   \n",
      "1              RI  Feature   Continuous        None  refractive index   \n",
      "2              Na  Feature   Continuous        None            Sodium   \n",
      "3              Mg  Feature   Continuous        None         Magnesium   \n",
      "4              Al  Feature   Continuous        None          Aluminum   \n",
      "5              Si  Feature   Continuous        None           Silicon   \n",
      "6               K  Feature   Continuous        None         Potassium   \n",
      "7              Ca  Feature   Continuous        None           Calcium   \n",
      "8              Ba  Feature   Continuous        None            Barium   \n",
      "9              Fe  Feature   Continuous        None              Iron   \n",
      "10  Type_of_glass   Target  Categorical        None              None   \n",
      "\n",
      "                                    units missing_values  \n",
      "0                                    None             no  \n",
      "1                                    None             no  \n",
      "2   weight percent in corresponding oxide             no  \n",
      "3   weight percent in corresponding oxide             no  \n",
      "4   weight percent in corresponding oxide             no  \n",
      "5   weight percent in corresponding oxide             no  \n",
      "6   weight percent in corresponding oxide             no  \n",
      "7   weight percent in corresponding oxide             no  \n",
      "8   weight percent in corresponding oxide             no  \n",
      "9   weight percent in corresponding oxide             no  \n",
      "10                                   None             no  \n"
     ]
    }
   ],
   "source": [
    "################################################### main starts here ############################################################\n",
    "\n",
    "######################################################### parse inputs ##########################################################\n",
    "\n",
    "# fetch dataset \n",
    "df = fetch_ucirepo(id=42) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = df.data.features \n",
    "y = df.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(census_income.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(df.variables) \n",
    "\n",
    "#print(\"data points:\", X)\n",
    "\n",
    "ucirepo_points = X.to_numpy()\n",
    "#print(\"data points in tuples:\", ucirepo_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8211fe78-2c1e-4189-8b76-802909e71626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative method 1: generate random 2D points\n",
    "# Generate random points\n",
    "np.random.seed(42)\n",
    "random_points = np.random.rand(200, 2) * 100  # 100 points in a 100x100 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48fc0556-1f06-4827-872d-173999fd8974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative Method 2: generate test points in clusters \n",
    "# Settings for the clusters\n",
    "n_samples = 200          # Total number of points\n",
    "n_features = 2           # Number of dimensions (2D)\n",
    "centers = 5              # Number of clusters\n",
    "cluster_std = 1        # Standard deviation of clusters\n",
    "cluster_points, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers, cluster_std=cluster_std, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93215d62-8a02-4628-950e-9b519ab62b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set this variable to one of random_points, cluster_points, or ucirepo_points\n",
    "#data_points = random_points\n",
    "data_points = cluster_points\n",
    "#data_points = ucirepo_points\n",
    "\n",
    "data_points = random.sample(list(data_points), 150)\n",
    "#random.shuffle(data_points)\n",
    "\n",
    "#print(data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "16f861ec-8386-4390-a202-716bebaaebd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################## set up dynamic streaming ###############################################\n",
    "# Preliminary simulation of dynamic streaming:\n",
    "# We'll add 20% of the amout of data to be removal requests\n",
    "# to simulate dynamic streaming.\n",
    "# For simplicity, whenever we encounter a removal request,\n",
    "# we randomly sample an active client point that is not in the set of centers\n",
    "# In our request array. a +1 indicates an insertion of a client;\n",
    "# -1 indicates a removal.\n",
    "requests = np.ones(int(len(data_points) * 1.1))\n",
    "removals = np.random.choice(range(0, len(data_points)+ 1), int(len(data_points)*0.2), replace=False)\n",
    "requests[removals] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb3a4e85-9ee8-428c-963b-3970ca3baf1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline OPT max distance: 5.233790503865138\n"
     ]
    }
   ],
   "source": [
    "############################################ solve the offline problem #################################################\n",
    "# Solve the offline k-center problem\n",
    "# (optional) greedy approximation of k-center for \n",
    "approx_centers = offline_k_center(data_points, k)\n",
    "max_dist_approx = max_distance_to_centers(data_points, approx_centers)\n",
    "\n",
    "# Offline LP relaxation to get OPT_dist\n",
    "# This value is used as OPT(t) in the online algorithm for each t\n",
    "max_dist = lp_relaxation_k_center(data_points, k)\n",
    "print(\"Offline OPT max distance:\", max_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404206b-89ec-41dd-8178-15be0094dc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################### run online algorithm including rouding ######################################\n",
    "\n",
    "fractional_sol, recourse, centers, OPT_rec, total_int_recourse = online_k_center(requests, data_points, k) \n",
    "\n",
    "# Output final results\n",
    "print(\"-----------final online results-----------\")\n",
    "print(\"k = \", k)\n",
    "print(\"beta = \", beta)\n",
    "print(\"epsilon = \", epsilon)\n",
    "print(\"alpha:\", alpha)\n",
    "print(\"number of centers (sum of fractional x's):\", np.sum(fractional_sol))\n",
    "print(\"OPT recourse:\", OPT_rec)\n",
    "#print(\"total fractional online recourse:\", recourse)\n",
    "print(\"final selected centers:\", centers)\n",
    "print(\"total rounding recourse:\", total_int_recourse)\n",
    "\n",
    "center_coordinates = random.sample(list(data_points), len(centers))\n",
    "for i in range(len(centers)):\n",
    "    center_coordinates[i] = data_points[centers[i]]\n",
    "max_online_dist = max_distance_to_centers(data_points, center_coordinates)\n",
    "\n",
    "print(\"max online distance:\", max_online_dist)\n",
    "\n",
    "print(\"alpha * beta * offline max distance:\", alpha * beta * max_dist)\n",
    "\n",
    "# Plot the points and the selected centers\n",
    "plot_points_and_centers(data_points, center_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f236fc-81d3-4ece-94a5-345c98e159b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
